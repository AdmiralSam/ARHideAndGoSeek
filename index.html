<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>AR Hide and Go Seek</title>
        <link rel="stylesheet" type="text/css" href="style.css"
    </head>
    <body>
        <header>AR Hide and Go Seek</header>
        <nav>
            <a href="index.html">Home</a>
            <a href="process.html">Process</a>
            <a href="algorithms.html">Algorithms</a>
            <a href="resources.html">Credits</a>
        </nav>
        <article>
            <h1>Purpose, Goals, and Outcomes</h1>
            
            <section>
                <h2>Purpose</h2>
                <p>
				The purpose of our project was to explore interactive augmented reality
				games that incorporated a larger degree of player motion. Using Occipital's
				Structure Sensor for state of the art tracking, we wanted to create a game
				that took the player's environment into account for increased interactivity.
                </p>
            </section>
            
            <section>
                <h2>Goals</h2>
                <ul>
					<li>Model-based tracking with Occipital's Structure Sensor</li>
					<li>Realtime occlusion using the depth sensor</li>
					<li>Lighting coherence between the real and virtual worlds</li>
					<li>Dynamic gameplay based on model and realtime information</li>
                </ul>
            </section>
            
            <section>
                <h2>Outcomes</h2>
                <p>
                We have two versions of the augmented reality hide and go seek game:
                </p>
				<ul>
					<li>A version that used the Structure Sensor for realtime tracking</li>
					<li>A version that simulated augmented reality with joysticks</li>
				</ul>
                <p>
				This was to allow simultaneous progress on both integrating the Structure
				Sensor and developing the AI and graphics.
				</p>
				<section>
					<h3>Structure Sensor Version</h3>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/8xw1vOYUH0k" frameborder="0" allowfullscreen></iframe>
					<p>
					In our Structure Sensor version, we were able to take pose information
					from the sensor and match it to our OpenGL world. We were also able to
					read the depth sensor's values, but we were not able to write them to
					OpenGL's depth buffer for realtime occlusion and visibility testing.
					</p>
				</section>
				
				<section>
					<h3>Simulated AR Version</h3>
					<iframe width="560" height="315" src="https://www.youtube.com/embed/CqPT7kAIFmI" frameborder="0" allowfullscreen></iframe>
					<p>
					In our simulated AR version, we were able to implement most of our game's
					functionality. This includes joystick control, visibility testing, basic
					hiding AI, animations, and basic lighting. We were not, however, able to 
					render real world shadows on our virtual objects.
					</p>
				</section>
            </section>
        </article>
    </body>
</html>