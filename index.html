<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>AR Hide and Go Seek</title>
        <link rel="stylesheet" type="text/css" href="style.css"
    </head>
    <body>
        <header>AR Hide and Go Seek</header>
        <nav>
            <a href="index.html">Home</a>
            <a href="index.html">Design</a>
            <a href="index.html">Algorithms</a>
            <a href="index.html">Credits</a>
        </nav>
        <article>
            
            <h1>Purpose, Goals, and Outcomes</h1>
            
            <section>
                <h2>Purpose</h2>
                <p>
				The purpose of our project was to explore interactive augmented reality
				games that incorporated a larger degree of player motion. Using Occipital's
				Structure Sensor for state of the art tracking, we could create a game that
				takes the player's environment into account for increased interactivity.
                </p>
            </section>
            
            <section>
                <h2>Goals</h2>
                <ul>
					<li>Model-based tracking with Occipital's Structure Sensor</li>
					<li>Realtime occlusion using the depth sensor</li>
					<li>Lighting coherence between the real and virtual worlds</li>
					<li>Dynamic gameplay based on model and realtime information</li>
                </ul>
            </section>
            
            <section>
                <h2>Outcomes</h2>
                <p>
                    We made two versions of the game:
                </p>
                
                <ul>
                    <li>Augmented reality version using the Structure Sensor</li>
                    <li>Simulated augmented reality version</li>
                </ul>
                
                <p>
                    The augmented reality version of the game which was our original goal uses the Structure Sensor to track. 
                    We prove that tracking works by rendering the model we created using the Matterport into the scene and when the user pans with the mobile device, the model pans accordingly.
                    We left the model in the scene because we were not able to get the depth buffer to work properly. We were able to read the data from the depth sensor, 
                    but we were unable to draw it into the buffer so we were not able to do real-time occlusion. 
                    Since we were unable to fill the depth data properly, Skitty always thinks she is hiding and never runs away to hide.
                </p>
                
                <p>
                    We also created a simulated augmented reality version of our game in which we do not do any tracking, but instead we render the model of the environment into the scene and
                    the user is able to move through it using two joysticks, one for orientation and one for position. 
                    We also have two more buttons, one shows and hides the visibility grid and the other one freezes and unfreezes Skitty to allow the player to take a closer look.
                    The visibility grid is used for the AI of Skitty and it stores the information of which grid cells are visible and invisible. 
                    We were able to implement skeletal animation and Lambertian shading, however we were not able to get shadow mapping fully working.
                </p>
                
            </section>
        </article>
    </body>
</html>