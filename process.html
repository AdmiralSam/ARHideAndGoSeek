<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>AR HGS - Design</title>
        <link rel="stylesheet" type="text/css" href="style.css"
    </head>
    <body>
        <header>AR Hide and Go Seek</header>
        <nav>
            <a href="index.html">Home</a>
            <a href="process.html">Process</a>
            <a href="algorithms.html">Algorithms</a>
            <a href="credits.html">Credits</a>
        </nav>
        <article>
            <h1>Development Process</h1>
            <section>
                <h2>iOS, OpenGL ES</h2>
                <p>
                    We were relatively new to programming for iOS, so our first step was to familiarize ourselves with programming in Objective-C in combination with C++.
                    We wanted to use C++ because we planned to use OpenCV to make our tracking more robust. We wanted to use model-based tracking for our application and had
                    created a 3D model of our classroom using the Matterport. One of the first steps was to render the classroom model onto our screen. For this we had to learn
                    how to use OpenGL ES which none of us had used before. We learned how to use shaders and wrote classes that would take care of all that for us to greatly 
                    simplify our code. We began refactoring our code early on because it made debugging easier and coding faster.
                </p>
            </section>
                
            <section>
                <h2>Attempted Tracking with Our Own Code</h2>
                <p>
                    A key element of virtual and augmented reality applications is tracking. Our original goal was to implement our own model-based tracking algorithm and only 
                    use the Structure Sensor's depth data for realtime occlusion. 
                    We tried using both feature detection and edge detection, combined with the initial pose from the chessboard, to obtain the camera pose.
                    We discovered, however, that our methods were not robust enough to be used in our application. Since we were already planning to use the Structure Sensor to obtain the depth data, we decided 
                    to also use it for tracking.
                </p>
                        
                <h2>Tracking with the Occipital Structure Sensor</h2>
                <p>
                    The Structure Sensor provides smooth, high-quality tracking using the device's camera and IMU (Inertial Measurement Unit). We started by simply trying to 
                    initialize the Structure Sensor from our code using the SDK, but we quickly ran into problems. We only had the examples that came with the SDK to help us, 
                    and the documentation was not very detailed which significantly slowed down our progress. 
                    
                    One of the biggest issues was not knowing the format of the data we were obtaining from the sensor because we did not know 
                    how to use it or modify it to fit our needs.
                </p>
            </section>
                
            <secton>
                <h2>An Alternative to Tracking</h2>
                <p>
                    We decided to implement another version of our project that would simulate augmented reality because of the issues
                    with the structure sensor, the fact that we only had one sensor, and the ability to work on the game part of our application simultaneously. 
                    We rendered the classroom model to the screen and added two joysticks, one for orientation and one for position, to allow movement through the environment.
                    We used inheritance and polymorphism to make switching in between the two different versions of our project easy (change one line of code).
                </p>          
            </secton>
            
            <section>
                <h2>Graphics</h2>
                <p>
                    The first thing we implemented was a visibility grid using the depth buffer from rendering the classroom model. The visibility grid provides Skitty with which locations the player cannot see. We then implemented skeletal animation, first with a simple snake with two bones, and finally with Skitty, which has 15 bones. We used the IQE (Inter-Quake Exchange) format for animation which came with a plugin for Blender. We attempted to implement shadow mapping and were able to get the shadow map, but had trouble applying it to objects.
                </p>
            </section> 
        </article>
    </body>
</html>