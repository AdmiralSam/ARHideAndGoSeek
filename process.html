<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <title>AR HGS - Design</title>
        <link rel="stylesheet" type="text/css" href="style.css"
    </head>
    <body>
        <header>AR Hide and Go Seek</header>
        <nav>
            <a href="index.html">Home</a>
            <a href="process.html">Process</a>
            <a href="algorithms.html">Algorithms</a>
            <a href="resources.html">Resources</a>
        </nav>
        <article>
            <h1>Development Process</h1>
            <section>
                <h2>iOS, OpenGL ES</h2>
                <p>
                    Being relatively new to iOS our first step was to familiarize ourselves with Objective-C in combination with C++ since we planned on using 
                    OpenCV to make our tracking more robust. We then learned to use OpenGL ES and shaders, which none of us had used before, and wrote classes 
                    to manage that for us and simplify our code. We began refactoring our code early on because it made debugging easier and coding faster.
                </p>
            </section>
                
            <section>
                <h2>Attempted Tracking with Our Own Code</h2>
                <p>    
                    A key element of any virtual and augmented reality application is tracking. Since we had already implemented simple tracking algorithms before, 
                    we hoped to implement a robust model-based tracking algorithm using a classroom model we made using the Matterport. We attempted both feature 
                    detection and edge detection, combined with the initial pose from the chessboard, to obtain the camera pose. We discovered, however, that our 
                    methods were not robust enough to be used in our application. Since we were already planning to use the Structure Sensor to obtain the depth 
                    data for realtime occlusion, we decided to also use it for tracking.
                </p>
                        
                <h2>Tracking with the Occipital Structure Sensor</h2>
                <p>
                    The Structure Sensor provides smooth, high-quality tracking using the device's camera and IMU (Inertial Measurement Unit). However using the 
                    Structure Sensor turned out to be challenging since we could not use a debugger. The port used for debugging was taken by the connection to 
                    the Structure Sensor and we only had the examples that came with the SDK to help us along with documentation that was not very detailed. We 
                    debugged our code by trial and error. The biggest issue we had was not knowing the format of the data we were obtaining from the sensor so that 
                    we could use it properly.
                </p>
            </section>
                
            <secton>
                <h2>An Alternative to Tracking</h2>
                <p>
                    We decided to implement another version of our project that would simulate augmented reality because of the issues with the Structure Sensor, 
                    the fact that we only had one sensor, and for the ability to work on the game part of our application simultaneously. For this version of our 
                    application we rendered the classroom model to the screen and added two joysticks, one for orientation and one for position, to allow movement 
                    through the environment. We used inheritance and polymorphism to make switching in between the two different versions of our project easy 
                    (change one line of code).
                </p>          
            </secton>
            
            <section>
                <h2>Graphics</h2>
                <p>
                    For our application, we did not want the graphics to break the illusion, which is why we wanted to focus on the details. We chose Skitty, a Pokemon
                    to be the creature that hides from the player. The first thing we implemented was a visibility grid using the depth buffer from rendering the 
                    classroom model. The visibility grid provides Skitty with which locations the player cannot see. We then implemented skeletal animation, first with 
                    a simple snake with two bones, and finally with Skitty, which has 15 bones. We used the IQE (Inter-Quake Exchange) format for animation which came 
                    with a plugin for Blender. We attempted to implement shadow mapping and were able to get the shadow map, but had trouble applying it to objects.
                </p>
            </section> 
        </article>
    </body>
</html>